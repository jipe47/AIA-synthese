\chapter{Biais et variance}

Dans le cadre de l'apprentissage supervisé, on cherche une fonction $f : \mathcal{X} \rightarrow \mathcal{Y}$ qui minimise l'erreur de généralisation, soit l'espérance

$$E_{x, y} \{L(y, f(x))\}$$

La fonction $L : \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ mesure la distance entre ses arguments :

\begin{itemize}
	\item en classification, $L(y, y') = 1$ si $y \neq y'$ (taux d'erreur)
	\item en régression, $L(y, y') = (y - y')^2$ (erreur quadratique).
\end{itemize}

Soit la fonction $\hat{f}_{LS}$ apprise à partir de $\LS$ à partir d'un algorithme d'apprentissage. Cette fonction (qui donne la prédiction à un certain point) est une variable aléatoire.

\dessin{24}

On peut définir l'erreur de généralisation, qui est utile pour caractériser et sélectionner un modèle :

$$Err_{LS} = E_{x, y} \{L(y, \hat{f}_{LS}(x))\}$$

Pour un algorithme d'apprentissage donné, l'erreur de généralisation sur des ensembles $\LS$ aléatoires de taille $N$ permet de caractériser un algorithme et est donnée par

$$E_{LS}\{Err_{LS}\} = E_{LS}\{E_{x, y} \{L(y, \hat{f}_{LS}(x))\} \}$$

	\section{Problème de régression sans variable d'entrée}
	
	Supposons que l'on cherche à prédire le mieux possible la taille d'un mâle adulte belge. On choisit une mesure de l'erreur, comme l'erreur quadratique, et on cherche une estimation $\hat{y}$ tel que l'espérance $E_y\{(y - \hat{y})^2\}$ sur toute la population belge soit minimale.
	
	\dessin{25}
	
	L'estimation qui minimise l'erreur peut être calculée en prenant 
	
	\begin{eqnarray*}
	 & \frac{\phi}{\phi y'} \ey{(y - y')^2} & = 0 \\
	\Leftrightarrow & \ey{-2(y - y')} & = 0 \\
	\Leftrightarrow & \ey{y} - \ey{y'} & = 0 \\
	\Leftrightarrow & y' & = \ey{y}
	\end{eqnarray*}
	
	Dans un modèle de Bayes, l'estimation qui minimise l'erreur est $\ey{y}$, soit la moyenne sur toute la population.
	
	En pratique, ce genre de valeur n'est pas calculable (il faudrait mesurer tous les adultes mâles belges).
	
	Vu que $p(y)$ est inconnu, on va trouver une estimation $\yh$ à partir d'un ensemble $LS = \ens{y_1, y_2, \dots , y_N}$, composé d'éléments tirés de manière aléatoire de la population. Autrement dit, on va chercher un algorithme d'apprentissage, par exemple
	
	\begin{enumerate}
		\item la moyenne : $\yh_1 = \frac{1}{N} \sum_{i = 1}^N y_i$
		\item en pondérant la valeur qu'on attend, avec $\lambda$ : $\yh_2 = \frac{\lambda . 180 + \sum_{i = 1}^N y_i}{\lambda + N}$, avec $\lambda \in [ 0, +\infty [ $
	\end{enumerate}
	
	\section{Décomposition biais/variance}
	
	Vu que $LS$ est tiré aléatoirement de la population, la prédiction $\yh$ est aussi une variable aléatoire. La distribution dépend de l'ensemble de l'apprentissage.
	
	\dessinS{26}{.5}
	
	Un bon algorithme d'apprentissage doit être bon sur un seul $LS$, mais aussi en moyenne sur plusieurs échantillons d'apprentissage. On veut minimiser 
	
	$$E = \els{\ey{(y - \yh)^2}}$$
	
	Si on ajoute et retire $\ey{y}$ et si on distribue le carré, on a
	
	\begin{eqnarray*}
  	E & = & \els{\ey{(y \textcolor{red}{- \ey{y} + \ey{y}} - \yh)^2}} \\
  	 & = & \els{\ey{(y - \ey{y})^2}} \\
  	 & + & \els{\ey{(\ey{y} - \yh)^2}} \\
  	 & + & \els{\ey{2(y - \ey{y})(\ey{y} - \yh)}}
	\end{eqnarray*}

	Or,
	
	\begin{itemize}
		\item $\els{\ey{(y - \ey{y})^2}} = \ey{(y - \ey{y})^2}$, car aucun terme ne dépend de $\LS$ (il n'y a que $\yh$ qui en dépend) ;
		\item $\els{\ey{(\ey{y} - \yh)^2}} = \els{(\ey{y} - \yh)^2}$ car $\ey{y}$ et $\yh$ sont des constantes dans l'expression, on peut donc les sortir du $\ey{.}$ qui les englobe.
	\end{itemize}
	
	\begin{eqnarray*}
  	E & = & \ey{(y - \ey{y})^2} \\
  	 & + & \els{(\ey{y} - \yh)^2} \\
  	 & + & \els{2\ey{(y - \ey{y})(\ey{y} - \yh)}}
	\end{eqnarray*}
	
	Or, $\ey{y - \ey{y}} = \ey{y} - \underbrace{\ey{\ey{y}}}_{\ey{y}}$, le troisième terme s'annule donc, ce qui conduit à
	
	$$E =\ey{(y - \ey{y})^2} + \els{(\ey{y} - \yh)^2}$$
	
	$\ey{(y - \ey{y})^2} = \vary{y}$ est la variance des données, c'est l'erreur résiduelle ; il s'agit de la plus petite erreur que l'on peut atteindre. Il s'agit d'une donnée du problème, on ne peut pas agir dessus.
	
	$\els{(\ey{y} - \yh)^2}$ est la moyenne sur tous les ensembles d'apprentissage de l'erreur du modèle de Bayes.
	
	%\dessinS{27}{.5}
	
	Si on suit le même raisonnement
	
	\begin{eqnarray*}
  	\els{(\ey{y} - \yh)^2} & = & \els{(\ey{y} \textcolor{red}{- \els{\yh} + \els{\yh}} - \yh)^2} \\
  	 & = & \els{(\ey{y} - \els{\yh})^2} \\
  	 & + & \els{(\els{\yh} - \yh)^2} \\
  	 & + & \els{2(\ey{y} - \els{\yh})(\els{\yh} - \yh)} \\
  	 & = & (\ey{y} - \els{\yh})^2 \\
  	 & + & \els{(\yh - \els{\yh})^2} \\
  	 & + & 2(\ey{y} - \els{\yh})(\els{\yh} - \els{\yh}) \\
  	 & = & (\ey{y} - \els{\yh})^2 + \els{(\yh - \els{\yh})^2}
	\end{eqnarray*}
	

	%\dessinS{28}{.5}
		
	$\els{\yh}$ est la moyenne du modèle sur tout l'échantillon $LS$. $(\ey{y} - \els{\yh})^2$ est le biais$^2$, l'erreur entre le modèle de Bayes et le modèle moyen.
		
	%\dessinS{29}{.5}
	
	$\varls{\yh}$ est l'estimation de la variance, qui est la conséquence de sur-apprentissage. On a que
	
	\begin{eqnarray*}
	E & = & \vary{y} + \text{biais}^2 + \els{(\yh - \els{\yh})^2} \\
     & = & \vary{y} + \text{biais}^2 + \varls{\yh}
	\end{eqnarray*}
	\dessinS{30}{.5}
	
	
	
	Pour les exemples :
	
	\begin{itemize}
		\item $\yh_1 = \frac{1}{N} \sum_{i = 1}^N y_i$
		
		$$\els{\frac{1}{N} \sum_{i = 1}^N y_i} = \frac{1}{N} \sum_{i = 1}^N E_{LS}\{y_i\} = \frac{1}{N} \sum_{i = 1}^N \ey{y} = \ey{y}$$
		
		$\els{y_i}$ est la taille moyenne d'une même personne pour plusieurs $LS$, tandis que $\ey{y}$ est la taille moyenne de la population : ce sont les mêmes choses, car la distribution est iid, le choix ne fait pas varier la taille.
		
		$$\varls{\yh} = \varls{\frac{1}{N} \sum_{i = 1}^N y_i} = \frac{1}{N^2} \varls{\sum_{i = 1}^N y_i}$$
		
		Par une propriété.
		
		$$= \frac{1}{N^2} \sum_{i = 1}^N \varls{y_i}$$
		
		car les $y_i$ sont indépendants.
		
		$$= \frac{1}{N^2} N \vary{y}$$
		
		car la distribution est iid.
		
		Les caractéristiques de cet estimateur sont que
		
		\begin{enumerate}
			\item si $N \rightarrow + \infty$, alors $E \rightarrow 0$.
			\item le biais est nul, c'est le meilleur estimateur.			
		\end{enumerate}
		
		\item $\lambda$ : $\yh_2 = \frac{\lambda . 180 + \sum_{i = 1}^N y_i}{\lambda + N}$, avec $\lambda \in [ 0, +\infty [ $
		
		$$\text{biais}^2 = \frac{\lambda}{\lambda + N} (\ey{y} - 180)^2$$
		
		$$\varls{\yh_2} = \frac{N}{(\lambda + N)^2} \vary{y}$$
		
		Caractéristiques :
		
		\begin{itemize}
			\item le biais est petit si la taille moyenne d'un homme belge est proche de 180
			\item plus l'échantillon est grand, plus le biais est petit
			\item si $\lambda$ est élevé, la variance va diminuer (car la prédiction devient constante), mais le biais va augmenter.
		\end{itemize}
	\end{itemize}
	
	$\yh_1$ et $\yh_2$ sont des estimateurs consistants car lorsque $N$ augmente (donc lorsqu'on a plus d'échantillons), $E$ diminue, ce qui n'est pas toujours le cas. $\yh_2$ combat le sur-apprentissage, on dit que $\lambda$ permet la régulation.
	
	
		\subsection{Approche bayesienne}
		
		Supposons que
		
		\begin{itemize}
			\item la taille moyenne est proche de 180 cm, suivant une loi normale :
			
			$$\pyo = A.exp(- \frac{(\yo - 180)^2}{2 \sigma_{\yo}})$$
			
			\item la taille d'une personne est gaussienne par rapport à la moyenne :
			
			$$P(y_i \vert \yo ) = B.exp(- \frac{(y_i - \yo)}{2 \sigma_y})$$
		\end{itemize}
		
		Quelle est la valeur la plus probable $\yo$ en connaissant $LS$ ?
		
		$$\yh = arg \: \text{max}_{\yo} P(\yo \vert LS)$$
		
		Voir slide 20.
	
		\subsection{Problème de régression (complet)}
	
		On considère qu'on cherche une fonction $\yh(x)$ à plusieurs entrées, on a une moyenne sur tout l'espace d'entrée.
	
		L'erreur devient
	
		\begin{eqnarray*}
		E & = & \els{\exy{(y - \yh(\xu))^2}} = \ex{\els{\eyx{(y - \yh(\xu))^2}}} \\
		 & = & \ex{\varyx{y}} + \ex{\text{biais}^2(\xu)} + \ex{\varls{\yh(\xu)}}
		\end{eqnarray*}
	
		On a finalement l'erreur suivante :
	
		$$\els{\eyx{(y - \yh(\xu))^2}} = \text{noise}(\xu) + \text{biais}^2(\xu) + \text{variance}(\xu)$$
	
		avec
	
		\begin{itemize}
			\item $\text{noise}(\xu) = \eyx{(y - h_B(\xu))^2}$, qui quantifie de combien $y$ varie de $h_B(\xu) = \eyx{y}$, le modèle de Bayes
			\item $\text{biais}^2(\xu) = (h_B(\xu) - \els{\yh(\xu)})^2$, qui mesure l'erreur entre le modèle de Bayes et la moyenne du modèle
			\item $\text{variance}(\xu) = \els{(\yh - \els{\yh(\xu)})^2}$, qui quantifie de combien $\yh(\xu)$ varie d'un échantillon d'apprentissage à un autre.
		\end{itemize}
	
		\subsection{Illustration}
		
		Soit une fonction $y = h(x) + \epsilon$, $\epsilon \approx N(0, 1)$.
		
		\dessinS{105}{.3}
		
		Si on utilise une méthode avec trop de biais et peu de variance, on a du sous-apprentissage. Le biais est la différence au carré entre les courbes bleu et noire (intégrée).
		
		\dessinS{106}{.3}
		
		Si on utilise une méthode avec peu de biais et beaucoup de variance, on a du sur-apprentissage. Le décalage est faible, mais la variance est haute. La variance est proche du bruit.
		
		\dessinS{107}{.3}
		
		A noter que ne pas avoir de bruit ne signifie pas qu'il n'y a pas de variance, mais qu'il y en a moins.
		
		\dessinS{108}{.3}

		\subsection{Lien entre biais/variance et sous/sur-apprentissage}
	
		Un modèle trop simple ne permet pas de capturer toute la complexité des données : on a un biais élevé.
	
		Un modèle trop complexe conduit à du sur-apprentissage : on a une variance élevée.
	
	
	\section{Problèmes de classification}
	
	L'erreur moyenne de mauvaise classification est
	
	$$E = \els{E_{\underline{x}, y}\{1 (y \neq \yh(\xu))\}}$$
	
	Le meilleur modèle possible est le modèle de Bayes :
	
	$$h_B(\xu) = \text{arg } \max_c P(y = c \vert \xu)$$
	
	Le modèle moyen est
	
	$$\text{arg } \max_c P(\yh(\xu) = c \vert \xu)$$
	
	Il n'y a pas de décomposition de l'erreur de mauvaise classification moyenne en un biais et une variance, mais on observe les mêmes phénomènes.
	
	Le biais est à peu près une composante de l'erreur qui est systématique et indépendante de l'échantillon d'apprentissage. La variance est l'erreur due à la variabilité du modèle par rapport à l'aspect aléatoire de l'échantillon d'apprentissage.
	
	\section{Paramètres influençant le biais et la variance}
	
		\subsection{Complexité du modèle}
		% Slide 31 ?
		
		Pour le problème considéré, on a que la variance du bruit vaut 1 et la moyenne est nulle : l'erreur résiduelle vaut donc 1.
		
		\dessinS{31}{.45}
		
		Généralement, le biais est une fonction décroissante de la complexité, tandis que la variance croît.
		
			\subsubsection{Réseaux de neurones}
		
			\dessinS{32}{.45}
			
			Plus on augmente le nombre de neurones dans la couche cachée et plus on obtient une fonction complexe. Du coup, on tient forcément plus compte de bruit ou d'outliers, ce qui entraîne une augmentation de la variance dans les prédictions. Le biais va diminuer pour la même raison ; on approxime de mieux en mieux la fonction.
		
			\subsubsection{Arbres de régression}
			
			\dessinS{33}{.45}
			
			Le raisonnement est similaire à celui des réseaux de neurones : plus l'arbre est profond et plus on tient compte du bruit et de cas isolés dans les données. De ce fait, on aura une erreur de resubstitution (donc le biais) moindre, mais la variance dans les prédictions sera plus grande.
			
			\subsubsection{$k$-NN}
			
			\dessinS{34}{.45}
			
			Plus $k$ est grand et plus on considère des voisins éloignés. Il est logique que la variance diminue, car on considère une fraction de plus en plus grande des données dans le calcul de la prédiction, du coup les prédictions varient de moins en moins (si on considérait tous les points, la prédiction serait la même tout le temps). En revanche, vu qu'on considère des voisins qui ont peu de chance d'être similaires à l'entrée, on aura un plus grand biais.
		
		\subsection{Bruit}
		
		A une complexité de modèle fixée, le biais augmente avec la complexité du modèle de Bayes. Cependant, les effets sur la variance sont difficiles à prédire.
		
		A cause du bruit, la variance augmente, mais globalement le biais n'est pas affecté. Par exemple, avec un arbre de régression (complet).
		
		\dessinS{35}{.45}
		
		\subsection{Taille de l'échantillon d'apprentissage}
		
		Pour les modèles dont la complexité ne dépend pas de la taille de $\LS$, le biais reste constant et la variance diminue avec la taille de l'échantillon. Par exemple, avec une régression linéaire.
		
		\dessinS{36}{.45}
		
		Lorsque la complexité du modèle est dépendante de la taille de l'échantillon d'apprentissage (par exemple les arbres), le biais et la variance décroissent avec la taille de l'échantillon d'apprentissage. Par exemple, avec un arbre de régression.
		
		\dessinS{37}{.45}
		
		\subsection{Algorithmes d'apprentissage}
		
		\dessinS{38}{.5}
		
		
		\begin{itemize}
			\item la régression linéaire possède peu de paramètres, donc une petite variance. Par contre, si la classification est non linéaire, on a une variance élevée
			\item $k$-NN : un petit $k$ implique une variance élevée et un biais modéré. Un grand $k$ implique une petite variance mais un biais plus grand
			\item réseau de neurones : le biais est petit, mais la variance augmente avec la complexité du modèle
			\item arbre de régression : le biais est petit, mais la variance est grande.
		\end{itemize}
	\section{Techniques de réduction du biais et de la variance}
	
	Afin de réduire le biais et la variance, on peut jouer sur les paramètres de l'algorithme d'apprentissage, cependant cela ne suffit pas toujours. On peut alors passer aux méthodes d'ensemble, qui offrent un trade-off biais/variance différent, mais qui font perdre des caractéristiques de la méthode initiale.
	
		\subsection{Réduction de la variance}
		
		L'idée générale est de réduire la capacité de l'algorithme d'apprendre $\LS$. Cela se fait par
		
		\begin{itemize}
			\item du pruning, afin de réduire explicitement la complexité de l'algorithme
			\item un arrêt précoce, afin de réduire la recherche
			\item une régularisation, qui réduit l'espace d'hypothèses. On utilise par exemple le weight decay avec les réseaux de neurones, qui consiste à pénaliser les grandes valeurs pour les poids.
		\end{itemize}
		
		\dessinS{98}{.45}
		
		La réalisation est la vraie valeur auquel s'ajoute du bruit. Le biais du modèle n'est pas équivalent au biais de l'algorithme d'apprentissage.
				
		Exemple de résultats :
		
		\dessinS{99}{.45}
		
		Comme attendu, la variance diminue mais le biais augmente.
		
		A noter qu'une bonne estimation du biais est l'erreur de resubstitution (sur $\LS$).