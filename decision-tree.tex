\chapter{Arbres de décision}

Il s'agit d'un algorithme d'apprentissage qui peut gérer les problèmes de classification (binaire ou avec plusieurs valeurs) et avec des attributs qui peuvent être discrets ou continus.

Un arbre de décision est un arbre où

\begin{itemize}
	\item chaque noeud intérieur teste un attribut,
	\item chaque branche correspond à la valeur d'un attribut, et
	\item chaque feuille est labellisée par une classe.
\end{itemize}

\dessin{16}

L'apprentissage avec cet algorithme consiste à choisir la structure d'arbre et à déterminer les prédictions aux feuilles.

Afin de minimiser l'erreur de classification, on associe aux feuilles la classe majoritaire lorsqu'on descend dans l'arbre.

\section{Arbre de classification}
	\subsection{Création d'un arbre de classification}
	
	On a l'algorithme suivant (Top-down induction of DT), pour une procédure \textit{learn\_dt($\LS$)}, où $\LS$ est l'échantillon d'apprentissage.
	
	\begin{itemize}
		\item[$\bullet$] si tous les objets de LS ont la même classe, créer une feuille avec cette classe comme label,
		\item[$\bullet$] sinon,
		\begin{itemize}
			\item trouver le meilleur attribut $A$ pour une séparation,
			\item créer un noeud de test pour cet attribut, et
			\item pour chaque valeur $a$ de $A$,
			\begin{itemize}
				\item[$\circ$] construire $LS_a = \ens{o \in LS \vert A(o) = a}$, et
				\item[$\circ$] utiliser \textit{learn\_dt($LS_a$)}, pour créer un sous-arbre à partir de $LS_a$.
			\end{itemize}
		\end{itemize}
	\end{itemize}
	
	Propriétés : 
	
	\begin{itemize}
		\item sous-optimal mais rapide
		\item dépend très fortement du critère pour sélectionner l'attribut
	\end{itemize}
	
	\subsection{Recherche du meilleur attribut}
	
	Pour trouver le meilleur attribut, il faut définir un score afin d'évaluer les séparations possibles. Ce score devra favoriser la séparation en classes, afin de réduire la profondeur de l'arbre.
	
	\dessin{17}
	
		\subsubsection{Impureté}
		
		Une mesure assez commune est l'impureté. Soit $p_j$ la proportion d'objets de la classe $j$ ($j = 1, \dots , J$) dans $\LS$.
		
		\begin{itemize}
			\item $I(\LS)$ est minimal si $p_i = 1$ et $p_j = 0$ pour $j \neq i$.
			\item $I(\LS)$ est maximal si $p_j = \frac{1}{J}$ : il y a le même nombre d'objets de chaque classe.
		\end{itemize}
		
		Le meilleur split est celui qui maximise la réduction d'impureté.
		
		$$\Delta I(\LS, A) = I(\LS) - \sum_a \frac{\vert \LS_a \vert}{\vert \LS \vert} I(\LS_a)$$
		
		avec $\LS_a$ le sous-ensemble des objets de $\LS$ tels que $A = a$. $\Delta I$ est une mesure de score ou un critère de séparation.
		
		%$$I(LS, A) = H(LS) - \frac{\vert LS_{\text{left}}\vert}{\vert LS\vert}H(LS_{\text{left}}) - \frac{\vert LS_{\text{right}}\vert}{\vert LS \vert}H(LS_{\text{right}})$$
		
		
		Exemple de mesure d'impureté :
		
		\begin{itemize}
			\item \textcolor{green}{entropie de Shannon} : $H(\LS) = \sum_j p_j \log_2 p_j$. Elle mesure l'incertitude, la surprise.
			
			\item \textcolor{blue}{indice de Gini} : $I(\LS) = \sum_j p_j(1 - p_j)$
			\item \textcolor{red}{taux d'erreur de mal-classification} : $I(\LS) = 1 - \max_j p_j$. En pratique, ce n'est jamais utilisé car cette mesure ne fait pas la différence entre des situations très différentes. Par exemple, une classification où on passe d'un score de 90\% à 100\% est bien plus intéressante qu'une classification qui passe de 50\% à 60\%, pourtant la différence d'impureté sera la même.
			
		\end{itemize}
		
		\dessin{88}
			
	\subsection{Élagage}

	Le sur-apprentissage survient lorsqu'on considère un arbre trop profond et qu'on tient compte de bruit ou de mauvaises mesures dans les données.
	
	\dessinS{18}{.4}
	
	Pour éviter l'overfitting, on a trois méthodes :
	
	\begin{itemize}
		\item pre-pruning/pré-élagage : arrêter d'étendre l'arbre plus tôt, avant qu'il n'atteigne le point où il classe parfaitement l'échantillon d'apprentissage ;
		\item post-pruning/post-élagage : permettre à l'arbre de sur-apprendre et de l'élaguer ensuite ;
		\item les méthodes d'ensemble.
	\end{itemize}
	
	
	
		\subsubsection{Pre-pruning}
		
		On arrête la séparation de noeud si
		
		\begin{itemize}
			\item le nombre d'objets est trop petit (\textit{min\_samples\_split}), ou si
			\item l'impureté est trop faible, ou si
			\item le meilleur test n'est pas statistiquement pertinent
		\end{itemize}
		
		Le problème est que le paramètre optimal pour cet élagage est très dépendant du problème, et que l'on peut passer à côté de l'arbre optimal.
		
		
		\subsubsection{Post-pruning}
		
		On sépare l'ensemble d'apprentissage $\LS$ en deux :
		
		\begin{itemize}
			\item un ensemble $\GS$ (growing sample) pour construire l'arbre, et
			\item un ensemble de validation $\VS$ pour évaluer l'erreur de généralisation.
		\end{itemize}
		
		On va construire un arbre complet à partir de $\GS$, puis on va calculer un ensemble d'arbres $\ens{T_1, T_2, \dots}$, avec $T_1$ l'arbre complet et $T_i$ l'arbre obtenu en retirant des noeuds de l'arbre $T_{i - 1}$.
		
		On sélectionne ensuite le $T_i*$ de la séquence qui minimise l'erreur sur $\VS$.
		
		\dessinS{19}{.4}
		
		Pour construire la séquence, on a plusieurs manières :
		
		\begin{itemize}
			\item reduced error pruning : à chaque étape, on retire le noeud qui diminue le plus l'erreur sur $\VS$
			\item cost-complexity pruning : on définit un critère de coût-complexité :
			
			$$\text{Error}_{\VS}(T) + \alpha \text{ Complexity}(T)$$
			
			On construit la séquence d'arbres qui minimise ce critère en augmentant $\alpha$.
		\end{itemize}
		
		
		Le problème du post-pruning est qu'il faut dédier une partie de l'ensemble d'apprentissage à la validation, ce qui peut poser un problème pour des petites bases de données. La solution est d'utiliser de la validation croisée $k$-fold.
	
	\subsection{Variables numériques}
	
	Deux solutions :
	\begin{itemize}
		\item pré-discrétiser, assigner des valeurs symboliques à des ranges (par exemple "froid" si la température est inférieure à 70$\,^{\circ}$F, "normal" si entre 70 et 75$\,^{\circ}$F, "chaud" si plus de 75$\,^{\circ}$F) ;
		\item discrétiser durant l'opération de construction de l'arbre, en prenant le seuil qui maximise l'impureté.
		\dessin{20}
	\end{itemize}
	
	\subsection{Attributs à plusieurs valeurs}
	
	Effectuer un split sur un attribut à plusieurs valeur n'est pas une bonne chose car il y a trop de fragmentation et la réduction d'impureté est souvent trop grande. Dès lors, on peut
	
	\begin{itemize}
		\item n'autoriser que les splits binaires
		\item dans le critère de choix du meilleur attribut, pénaliser les valeurs multiples
	\end{itemize}
	
	\subsection{Valeurs d'attribut manquantes}
	
	On peut soit
	\begin{itemize}
		\item assigner la valeur la plus fréquente dans $\LS$
		\item assigner la valeur la plus fréquente dans l'arbre
		\item assigner une probabilité à chaque valeur possible
	\end{itemize}
	
\section{Arbre de régression}
	
	Un arbre de régression est un arbre de décision où les labels des noeuds sont numériques.
	

	\dessinS{89}{.5}
	
	Afin de minimiser l'erreur quadratique sur $\LS$, la prédiction à une feuille est la moyenne de tous les éléments de $\LS$ qui l'atteignent.
	
	L'impureté d'un échantillon est définie par la variance de la sortie de cet échantillon :
	
	$$I(\LS) = \text{var}_{y \vert \LS} \ens{y} = \text{E}_{y \vert \LS}\ens{(y - \text{E}_{y \vert \LS}\ens{y})^2}$$
	
	Le meilleur split est celui qui diminue le plus la variance :
	
	$$\Delta I(\LS, A) =\text{var}_{y \vert \LS} \ens{y} - \sum_a \frac{\vert \LS_a \vert}{\vert \LS \vert} \text{var}_{y \vert \LS_a} \ens{y}$$

	L'élagage fonctionne de la même façon que pour les arbres de classification, si ce n'est que dans le post-pruning, c'est l'arbre qui minimise l'erreur quadratique sur $\VS$ qui est sélectionné.
	
	En pratique, le pruning est plus important en régression car les arbres complets sont beaucoup plus complexes ; souvent, tous les objets ont une sortie différente, du coup l'arbre complet a autant de feuilles qu'il y a d'objets dans l'échantillon d'apprentissage.


\section{Interprétabilité et sélection d'attribut}
	
	Un arbre de décision est très interprétable, il peut être converti facilement en un ensemble de règles "si \dots alors".
	
	Si certains attributs ne sont pas nécessaires pour la classification, il n'apparaitront pas dans l'arbre (élagé/pruned). C'est important si la mesure de certaines variables est coûteuse.
	
	Les arbres de décision sont souvent utilisés comme pre-processing pour d'autres algorithmes d'apprentissage, qui souffrent de variables inutiles.
	
	Certaines variables ont une importance, elles ne contribuent pas toutes de manière égale. Grâce aux arbres, on peut évaluer leur importance. Cela peut permettre notamment d'éviter de mesurer des attributs si cela est coûteux et inutile.
	
	L'importance d'une variable est calculée par la réduction de l'impureté :
	
	$$I(A) = \sum_{\substack{\text{noeuds où} \\ \text{A est testé}}} \vert \LS_{\text{noeud}} \vert \Delta I(\LS_{\text{noeud}}, A)$$
	
	
	\section{Avantages et inconvénients}
	
	\begin{itemize}
		\item[+] très rapide et scalable, on peut traiter d'énorme quantité d'entrées et d'objets ; la complexité est de l'ordre $\bigoh (nN\log N)$
		\item[+] donne une bonne interprétabilité et quantifie l'importance des variables ;
		\item[+] très flexible : support de plusieurs types d'attribut, de valeurs manquantes, de problèmes de classification et de régression, etc
		\item[-] grande variance et donc grande instabilité ;
		\item[-] souvent pas aussi précis que d'autres méthodes.
	\end{itemize}
	