\chapter{Machines à support vectoriel}

Cette méthode se base sur deux idées :

\begin{enumerate}
	\item la mise en place d'un classificateur à large marge, et
	\item le "noyautage" de l'espace d'entrée.
\end{enumerate}

Il faut trouver un classificateur linéaire. L'idée va être de trouver celui qui maximise la marge, c'est-à-dire la largeur du bord qui peut être étendu jusqu'à toucher une donnée.

\dessinS{15}{.45}

\section{Machines à support vectoriel linéaire}
	
	Soit un $\LS = \ens{(x_k, y_k)}^N_{k = 1}$, avec $y_k \in [-1, 1]$ et $x_k \in \mathbb{R}^n$. On cherche un classificateur de la forme
	
	$$\yh(x) = sgn(w^T x + b)$$
	
	qui classifie $\LS$ correctement, c'est-à-dire qui minimise
	
	$$\sum_{k = 1}^N1(y_k \neq \yh(x_k))$$
	
	\subsection{Hyperplan de marge maximale}
	
	Lorsque les données sont linéairement séparables dans l'espace des features, l'hyperplan séparateur n'est pas unique.
	
	\dessin{55}
	
	Une SVM va chercher à maximiser la distance de l'hyperplan au point le plus proche dans $\LS$, autrement dit
	
	$$\underbrace{\max_{w, b} \underbrace{min \ens{\Vert x - x_k \Vert : w^T x + b = 0, k = 1, \dots , N}}_{\text{point le plus proche de la droite}}}_{\text{maximisation de la marge}}$$
	
	On maximise la marge car, intuitivement, c'est une méthode sûre. De plus, il existe des bornes théoriques sur l'erreur de généralisation qui dépend de la marge :
	
	$$Err(TS) < \bigoh (\frac{1}{\gamma})$$
	
	où $\gamma$ est la marge. Cependant, ces marges ne sont pas souvent atteintes. En pratique, une SVM fonctionne très bien. A noter qu'on a toujours deux points qui sont les plus proches de la droite. C'est toujours le cas, sinon on pourrait encore augmenter la marge.
	
	Cet algorithme conduit à un problème d'optimisation convexe, où la solution peut être écrite uniquement en terme de produits scalaires.
	
	\subsection{Problème d'optimisation}
	
	\dessin{56}
		
	Supposons un hyperplan séparateur (ligne rouge) et un vecteur de support $x$. Ce dernier peut s'écrire	
	
	$$x = x_{\perp} + r \frac{w}{\Vert w \Vert}$$
	
	où $x_{\perp}$ est la projection de $x$ sur l'hyperplan et $\vert r \vert$ la distance entre $x$ et l'hyperplan. $r$ est donc la marge. En multipliant les deux membres par $w^T$, on obtient
	
	$$w^Tx = \underbrace{w^T x_{\perp}}_{\substack{-b\\ \text{car } x_{\perp} \text{ est sur} \\ \text{la droite}}} + r \frac{\overbrace{w^T w}^{\Vert w \Vert2}}{\Vert w \Vert}$$
	$$\Leftrightarrow r = \frac{w^Tx + b}{\Vert w \Vert} = \frac{y(x)}{\Vert w \Vert}$$
	
	Le problème d'optimisation qui consiste à maximiser la marge peut alors être écrit comme 
	
	$$arg \max_{w, b} \ens{\frac{1}{\Vert w \Vert} \min_{n} [y_n . (w^T x_n + b)]}$$
		
	La solution n'est pas unique vu que l'hyperplan est inchangé si  on multiplie $w$ et $b$ par une constante $c > 0$ (par exemple, $c (w^Tx + b) = 0$ est aussi une solution). 
	
	Pour imposer une unicité et pour normaliser, on choisit typiquement $\vert w^T x + b \vert = 1$ pour le point $x$ qui est le plus proche de la surface, autrement dit pour le vecteur de support. On a alors que pour tous les points $k = 1, \dots , N$ :
	
	$$y_k(w^T x_k + b) \geq 1$$
	
	Grâce à cette normalisation, la marge vaut désormais $\frac{1}{\Vert w \Vert}$, qu'il faut maximiser (ou minimiser $\Vert w \Vert$) avec $N$ contraintes
		
	$$\min_{w, b} \varepsilon(w, b) = \frac{1}{2} \Vert w \Vert^2$$
	
	sujet aux $N$ contraintes
	
	$$y_k(w^T x_k + b) \geq 1, \: \forall k = 1, \dots , N$$

	$\Vert w \Vert$ devient $\frac{1}{2} \Vert w \Vert^2$ par facilité (le $\frac{1}{2}$ permet de simplifier lors de la dérivation). Il s'agit d'un problème de programmation quadratique. Il existe une solution seulement si les données sont linéairement séparables, sinon des inéquations ne seront pas satisfaites.
	
	%Optimisation des contraintes : 10 $\rightarrow$ 17/45
	
	Soient $\alpha_k$, $k = 1, \dots N$. On a le lagrangien qui doit être minimisé selon $w$ et $b$ et maximisé selon $\alpha$ :
	
	$$\mathcal{L}(w, b, \alpha) = \frac{1}{2} \Vert w \Vert^2 - \sum_{k = 1}^N \alpha_k (y_k (w^Tx_k + b) - 1)$$
	
	Si on dérive par rapport à $w$ et $b$, on obtient les conditions optimales suivantes :
	
	\begin{eqnarray*}
	\frac{\vartheta \mathcal{L}}{\vartheta w} = 0 & \Leftrightarrow & \sum_{j = 1}^N \alpha_j y_j x_j = w \\
	\frac{\vartheta \mathcal{L}}{\vartheta b} = 0 & \Leftrightarrow & \sum_{j = 1}^N \alpha_j y_j = 0
	\end{eqnarray*}
	
	Après substitution dans le lagrangien, les conditions optimales conduisent au problème de maximisation dual :
	
	$$\max_{\alpha} \mathcal{W}(\alpha) = \sum_{k = 1}^N \alpha_k - \frac{1}{2} \sum_{i, j = 1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j$$
	
	sujet aux $N$ inégalités
	
	$$\alpha_k \geq 0, \: \forall k = 1, \dots, N$$
	
	et à l'égalité
	
	$$\sum_{i = 1}^N \alpha_i y_i = 0$$
	
	
	\subsection{Vecteurs de support}
	
	Le problème primaire est donc
	
	$$\mathcal{L}(w, b, \alpha) = \frac{1}{2} \Vert w \Vert - \sum_{k = 1}^N \alpha_k (y_k (w^Tx_k + b) - 1)$$
	
	En accord avec les conditions complémentaires KKT (Karush-Kuhn-Tucker), le vecteur solution $w$ est tel que
	
	$$\alpha_k (y_k (w^T x_k + b) - 1) = 0, \: \forall k = 1, \dots , N$$
	
	\begin{itemize}
		\item $\alpha_k = 0$ si la contrainte est satisfaite comme une inégalité stricte $y_k (w^T x_k + b) > 1$, car c'est la façon de maximiser $\mathcal{L}$.
		
		\item $\alpha_k > 0$ si la contrainte est satisfaite comme une égalité $y_k(w^Tx_k + b) = 1$, auquel cas $x_k$ est le vecteur de support.
	\end{itemize}
	
	Les seuls points pour lesquelles les contraintes du lagrangie sont actives sont donc les points tels que $y_k (w^T x_k + b) = 1$, autrement dit les vecteurs de support : seuls eux définissent l'hyperplan optimal.
	
	Une fois que les valeurs optimales de $\alpha$ ont été déterminées, le modèle final peut être écrit comme
	
	$$\yh(x) = sgn(\sum_{i= 1}^N y_i \alpha_i x_i^T x + b)$$
	
	où les valeurs de $\alpha_k$ différentes de 0 (strictement positives) correspondent aux vecteurs de support.
	
	$b$ est calculé en exploitant le fait que pour tout $\alpha_k > 0$, on a nécessairement $y_k(w^Tx_k + b) - 1 = 0$.
	
	%Borne pour le leave-one-out : 20/45
	
	Avoir un petit ensemble de vecteurs de support est efficace, vu que seuls ces vecteurs $x_k$, leurs poids $\alpha_k$ et les labels de classe $y_k$ doivent être stockés pour classer de nouveaux exemples.
	
	Si un vecteur non-support $x'$ est retiré de l'échantillon d'apprentissage ou déplacé (en dehors de la région de la marge), la solution est inchangée. De même, elle ne change pas si on ajoute des points qui ne sont pas des vecteurs de support : les SVMs sont peu sensibles aux changements du $\LS$.
	
	La proportion de vecteurs de support dans l'ensemble d'échantillon donne une borne sur l'erreur du leave-one-out :
	
	$$\text{Err}_\text{loo} \leq \frac{\vert \ens{k \vert \alpha_k > 0} \vert}{N} = \frac{\sharp \text{ vecteur de support}}{N}$$
	
	\subsection{Rappel du lagrangien}
	
	Supposons que l'on veuille minimiser une fonction $f(x) \in \mathbb{R}$ avec $x \in \mathcal{R}^n$ sujette aux égalités
	
	$$e_i(x) = 0, \: i = 1, \dots , p$$
	
	et aux inégalités
	
	$$i_j(x) \leq 0, \: j = 1, \dots , r$$
	
	Le lagrangien est défini par
	
	$$\mathcal{L}(x, \alpha, \beta) = f(x) + \alpha^T e(x) + \beta^T i(x), \: \alpha \in \mathbb{R}^p, \: \beta \in \mathcal{R}^r$$
	
	Aux optima, les conditions suivantes sont vérifiées :
	
	\begin{eqnarray*}
	\Delta_x \mathcal{L} = 0 & \rightarrow & \Delta_x f(x) = \alpha^T \Delta_x e(x) + \beta^T \Delta_x i(x) = 0 \\
	\Delta_{\alpha} \mathcal{L} = 0 & \rightarrow & e_i(x) = 0, \: \forall i = 1, \dots , p \\
	\Delta_\beta \mathcal{L} \leq 0 & \rightarrow & i_j(x) \leq 0, \: \beta_j i_j(x) = 0, \: \beta_j \geq 0, \: \forall j = 1, \dots , r
	\end{eqnarray*}
		
	\subsection{Soft margin}
	
	A cause du bruit ou de données isolées (outliers), les données peuvent ne pas être linéairement séparables dans l'espace des features. 
	
	\dessin{57}
	
	Les discordances sont mesurées par les variables $\xi_i \geq 0$ avec la contrainte relaxée associée $y_i(w^Tx_i + b) \geq 1 - \xi_i$. En rendant $\xi_i$ suffisamment large, la contrainte peut toujours être satisfaite :
	
	\begin{itemize}
		\item si $0 < \xi_i < 1$, la marge n'est pas satisfaite mais $x_i$ est toujours correctement classé
		\item si $\xi_i > 1$, alors $x_i$ est mal classé.
	\end{itemize}
	
	\subsubsection{Marge douce en norme 1}
	
	Le problème primal est
	
	$$\min_{w, \xi} \frac{1}{2} \Vert w \Vert^2 + C \sum_{i = 1}^N \xi_i$$
	s.t.
	
	$$y_i(w^T x_i + b) \geq 1 - \xi_i, \: \xi_i \geq 0, \: \forall i = 1, \dots , N$$
	
	où $C$ est une constante positive et qui équilibre l'objectif de maximiser la marge et de minimiser l'erreur engendrée. Ainsi, si $C = 0$, on ne pénalise pas les $\xi_i$ donc les mauvais classements. Plus $C$ est grand et plus on augmente la complexité, car les $\alpha_k$ peuvent être grands et on veut un meilleur classement (donc on prend de plus en plus en compte les $\xi_i$).
	
	\dessin{77}
	
	On a le problème dual :
	
	$$\max_\alpha \mathcal{W}(\alpha) = \sum_{k = 1}^N \alpha_k - \frac{1}{2} \sum_{i, j = 1}^N \alpha_i \alpha_j y_i y_j x_i^T x_j$$
	
	s.t.
	
	$$0 \leq \alpha_k \leq \textcolor{red}{C}, \: \forall k = 1, \dots , N \text{ (contraintes de boîte)}$$
	$$\sum_{i = 1}^N \alpha_i y_i = 0$$
	
	\dessinS{58}{.5}
	
	Les points dans la marge correspondent à des $\xi_i > 0$. La figure du milieu illustre du sur-apprentissage.
	
\section{Noyaux dans les SVMs}

Les données d'apprentissage peuvent ne pas être linéairement séparables dans l'espace d'entrée. On considère alors un mapping non linéaire $\phi$ vers un nouvel espace de features.

\dessin{59}

L'hyperplan est donc défini par

$$y(x) = w^T \phi(x) + b$$

Le problème dual devient

$$\max_\alpha \mathcal{W}(\alpha) = \sum_{k = 1}^N \alpha_k - \frac{1}{2} \sum_{i, j = 1}^N \alpha_i \alpha_j y_i y_j \phi(x_i)^T \phi(x_j)$$

Plutôt que de définir le mapping $\phi$, on peut directement définir le produit scalaire $K(x, x') = \phi(x)^T \phi(x')$, ce qui rend le mapping implicite.

Il est possible de caractériser mathématiquement les fonctions $K(x, x')$ définies sur des paires d'objets : cette fonction est appelée un noyau (positif ou de Mercer). Il peut cependant être difficile de trouver une mesure de la similarité entre $x$ et $x'$.

L'hyperplan séparateur est alors défini par

$$y(x) = \sum_{k = 1}^N \alpha_k y_k K(x_k, x) + b$$

Le modèle devient alors

$$\yh(x) = sgn(\sum_{k = 1}^N y_k \alpha_k K(x, x_k) + b)$$

où les $\alpha_k$ peuvent être déterminés en résolvant le problème de maximisation quadratique 

$$\max_\alpha \mathcal{W}(\alpha) = \sum_{k = 1}^N \alpha_k - \frac{1}{2} \sum_{i, j = 1}^N \alpha_i \alpha_j y_i y_j K(x_i, x_j)$$

sujet aux $N$ contraintes d'inégalité

$$\alpha_k \geq 0, \: \forall k = 1, \dots , N$$

et la contrainte d'égalité

$$\sum_{i = 1}^N \alpha_i y_i = 0$$

\section{Kernel trick}

Le kernel trick consiste à transformer l'espace d'entrée en un autre espace où l'algorithme se comporte mieux/est plus rapide. N'importe quel algorithme qui utilise les données avec uniquement des produits scalaires peut se baser sur ce mapping implicite, en remplaçant $x^Tx'$ par $K(x, x')$. 

Les avantages sont que

\begin{itemize}
	\item on a un calcul moins coûteux qu'un produit scalaire en grande dimension
	\item la transformation $\phi$ est implicite
\end{itemize}

\section{Notion mathématique du noyau}

Soit $U$ un ensemble d'objets non vide. Un noyau positif est une fonction $K(., .)$

$$K(.,.) : U \times U \rightarrow \mathbb{R}$$

telle que pour tout $N \in \mathbb{N}$ et pour tout $o_1, \dots , o_N \in U$, la matrice $N \times N$

$$K : K_{i, j} = K(o_i, o_j)$$

est symétrique et définie semi-positive, c'est-à-dire que ses valeurs propres sont positives, ou bien que $z^T K z \geq 0, \: \forall z \in \mathbb{R}^n$.

Pour tout noyau positif $K$ défini sur $U$, il existe un espace de produit scalaire $\mathcal{V}$ et une fonction $\phi(.) : U \rightarrow \mathcal{V}$ tel que

$$K(o, o') = \phi(o) \times \phi(o')$$

où l'opérateur $\times$ dénote le produit scalaire dans $\mathcal{V}$.

En général, l'espace $\mathcal{V}$ n'est pas nécessairement de dimension finie.

Le noyau défini un produit scalaire, et donc une norme et une mesure de distance sur $U$, qui est héritée de $\mathcal{V}$ :
\begin{eqnarray*}
d^2_U(o, o') & = & d^2_\mathcal{V}(\phi(o), \phi(o')) \\
 & = & (\phi(o) - \phi(o'))^T(\phi(o) - \phi(o')) \\
 & = & \phi(o) \times \phi(o) + \phi(o') \times \phi(o') - 2 \phi(o) \times \phi(o') \\
 & = & K(o, o) + K(o', o') - 2 K(o, o')
\end{eqnarray*}

\section{Exemple de noyaux}

\begin{itemize}
	\item noyau constant : $K(o, o') = 1$
	\item noyau linéaire défini sur des attributs numériques : $K(o, o') = \mathbf{a}^T(o) \mathbf{a}(o') = \langle \mathbf{a}(o), \mathbf{a}(o')\rangle$ (produit scalaire)
	\item noyau polynomial : si $d$ est le degré maximum $K(o, o') = (\langle \mathbf{a}(o), \mathbf{a}(o') \rangle + 1)^d$
	\item noyau de Hamming pour des attributs discrets : $K(o, o') = \sum_{i = 1}^m \delta_{\mathbf{a}_i(o), \mathbf{a}_i(o')}$ (nombre de composantes communes aux deux vecteurs)
	\item noyau de texte, qui calcule le nombre de sous-chaînes communes dans $o$ et $o'$ (dimension infinie si la taille du texte n'est à priori pas bornée)
	\item combinaison de noyaux :
	
	\begin{itemize}
		\item la somme de plusieurs noyaux (positifs) est toujours un noyau (positif)
		\item le produit de plusieurs noyaux est aussi un noyau
		\item noyaux polynomiaux : $\sum_{i = 0}^n a_i(K(x, x'))^i$ si $\forall i : a_i \geq 0$
	\end{itemize}
	
	\item $K(x, x') = (x^Tx')^2$ (voir slide 31/45 pour plus de détails)
	\item noyau gaussien/à base radiale : si $\sigma$ est l'étalement de la distribution, $K(x, x') = \exp{\frac{-(x - x')^T(x - x')}{2 \sigma^2}}$
\end{itemize}

\dessinS{60}{.55}

\section{Méthodes à noyaux}

\dessin{61}

L'approche est modulaire : on découple l'algorithme de la représentation. Beaucoup d'algorithmes peuvent utiliser des noyaux : ridge regression, PCA, k-means, etc. Des noyaux ont été définis pour plusieurs types de données : séries temporelles, images, graphes, séquences, etc.

Les intérêts principaux des noyaux est que l'on peut travailler efficacement dans des espaces de dimension très élevée (potentiellement infinie) dès qu'un produit scalaire est facile à calculer, et qu'on peut appliquer des algorithmes classiques sur des données en toute généralité, sans être nécessairement vectorielles (par exemple construire un modèle linéaire sur un graphe).

\section{Forces et faiblesses des SVMs}

\begin{itemize}
	\item[+] les SVMs sont motivées théoriquement
	\item[+] classificateur des plus efficaces
	\item[+] implémentations efficaces pour des problèmes larges
	\item[-] modèles en boîte noire
	\item[-] le choix d'un bon noyau est difficile et critique pour atteindre une bonne précision
\end{itemize}

L'optimisation convexe est un outil très utile en apprentissage, car beaucoup de problèmes peuvent être formulés comme des problèmes d'optimisation convexe. On bénéficie également du travail important donné pour créer des algorithmes d'optimisation efficaces, et qui plus est donnent une solution unique (ce qui rend le problème plus stable).