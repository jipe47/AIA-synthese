\chapter{Apprentissage non supervisé}

Le but de l'apprentissage non supervisé est de trouver des régularités dans les données sans se soucier de la relation entrée-sortie. On recherche ainsi les groupes de variables ou d'objets intéressants et des dépendances entre les variables.

Il existe trois grandes familles de problèmes :

\begin{itemize}
	\item clustering : trouver des groupes d'échantillons ou de variables.
	\item réduction de dimensionnalité : on projette les données d'un espace à haute dimension vers un espace plus petit.
	\item estimation de densité : déterminer la distribution des données dans l'espace d'entrée.
\end{itemize}

\section{Clustering}

Le but est de grouper une collection d'objets en sous-ensembles (appelés clusters), de façon à ce que chaque objet dans un cluster soit proche des autres, tout en étant éloigné des objets des autres clusters.
	
Ces groupements peuvent être

\begin{itemize}
	\item des groupements de lignes/d'objets similaires
	\item des groupements de colonnes/variables
	\item du bi-clustering, c'est-à-dire en se basant sur les lignes et les colonnes.
\end{itemize}

\dessin{43}

Applications :

\begin{itemize}
	\item marketing : trouver des groupes de clients qui ont un comportement similaire, en se basant sur leurs caractéristiques et les achats précédents
	\item biologie : classifier la faune et la flore selon leurs caractéristiques
	\item web : classification de documents (par exemple des articles de blog)
\end{itemize}

Deux composantes distinctes sont considérées :

\begin{itemize}
	\item la mesure de distance entre deux objets
	\item un algorithme de clustering, qui va minimiser les distances entre les objets d'un groupe et/ou maximiser les distances entre des groupes
\end{itemize}

	\subsection{Mesure de distances}
		\subsubsection{Distance Euclidienne}
		Elle mesure la différence entre des coordonnées et pénalise les grosses différences. Il s'agit de la racine carrée de la somme des carrés des différences entre les coordonnées :
		
		$$d_e(x_1, x_2) = \sqrt{(x_{1,0}-x_{2,0})^2 + (x_{1,1}-x_{2,1})^2 + \dots}$$
		
		\subsubsection{Distance de Manhattan}
		Elle mesure la différence entre des coordonnées, mais de manière robuste. Il s'agit de la somme des différences absolues de toutes les coordonnées :
		
		$$d_e(x_1, x_2) = \vert x_{1,0}-x_{2,0} \vert + \vert x_{1,1}-x_{2,1} \vert + \dots$$
		
		\subsubsection{Corrélation}
		Elle mesure une différence en tenant compte des tendances. La distance entre deux vecteurs est $(1 - \rho)$, où $\rho$ est la corrélation de Pearson entre les deux vecteurs :
		
		$$\rho(x_1, x_2) = \frac{cov(x_1, x_2)}{\sigma_{x_1} \sigma_{x_2}} = \frac{\sum_{i = 1}^n (x_{1, i} - \overline{x}_1)(x_{2, i} - \overline{x}_2)}{\sqrt{\sum_{i = 1}^n (x_{1, i} - \overline{x}_1)^2} \sqrt{\sum_{i = 1}^n (x_{2, i} - \overline{x}_2)^2}}$$
	
		On a que $\rho \in [-1, 1]$, donc $(1 - \rho) \in [0, 2]$ : 0 signifie que les données sont fortement corrélées.
	
	\subsection{Clustering hiérarchique}
	
		\subsubsection{Algorithme}
		On a l'algorithme suivant :
	
		\begin{enumerate}
			\item Chaque objet est assigné à son propre cluster
			\item Itérativement :
		
			\begin{itemize}
				\item les deux clusters les plus similaires sont joins et rassemblés en un.
				\item la matrice de distances est mise à jour avec le nouveau cluster qui en remplace deux.
			\end{itemize}
		\end{enumerate}
				
		\dessinS{45}{.5}
		
		\subsubsection{Distance entre deux clusters}
		
		On a plusieurs possibilités :
		
		\begin{itemize}
			\item \textcolor{red}{Single linkage} : utiliser la plus petite distance entre deux objets du cluster. Cela a tendance à créer des clusters étalés.
			
			$$d_S(G, H) = \min_{i \in G, j \in H} d_{ij}$$
			
			\item \textcolor{blue}{Complete linkage} : utiliser la plus grande distance entre deux objets du cluster. Cela a tendance à créer des grappes.
			
			$$d_C(G, H) = \max_{i \in G, j \in H} d_{ij}$$
			
			\item \textcolor{green}{Average distance} : calculer la distance moyenne. On obtient un mix des deux autres mesures ; on a une sorte de distance entre les centres de masse.
						
			$$d_A(G, H) = \frac{1}{N_GN_H} \sum_{i \in G} \sum_{j \in H} d_{ij}$$
			
		\end{itemize}
				
		\dessin{44}
		
		
		\subsubsection{Dendrogramme}
		
		Cela permet de visualiser le clustering hiérarchique et de déterminer visuellement le nombre de clusters. Les clusters qui sont unis sont combinés par une ligne. La hauteur d'une ligne est la distance entre les clusters.
		
		\dessin{46}
		
		
		\subsubsection{Forces et faiblesses}
		
		\begin{itemize}
			\item[+] on n'a pas besoin de supposer un nombre particulier de cluster
			\item[+] on peut utiliser n'importe quel type de matrice de distance
			\item[+] on a parfois une interprétation facile des résultats
			
			\item[-] trouver une interprétation n'est pas toujours aisé
			\item[-] une fois qu'il a été décidé de combiner deux clusters, on ne peut pas revenir en arrière. Par exemple, le cluster rouge montre un mauvais départ, alors qu'on aurait voulu obtenir les deux clusters verts :
			
			\dessin{47}
			\item[-] pas très bien motivé théoriquement
		\end{itemize}
		
		\subsubsection{Algorithme de clustering combinatoire}
		
		Soit un nombre de clusters $K < N$ et un encodeur $C$ qui assigne la $i$ème observation au cluster $C(i)$. On va chercher la fonction $C^*$ qui minimise une fonction de perte, qui mesure si l'objectif de clustering est atteint.
		
		Par exemple, on pourrait avoir comme fonction de perte une qui se base sur l'éparpillement des objets d'un cluster (within cluster scatter) :
		
		$$W(C) = \frac{1}{2} \sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(i') = k} d(x_i, x_{i'})$$
		
		Plus les points d'un cluster sont rapprochés, plus ce nombre est petit et donc plus les clusters sont meilleurs.
		
		Le nombre de possibilité est cependant trop grand pour une énumération.
		
		$$S(N, K) = \frac{1}{K!} \sum_{k = 1}^K(-1)^{K - k} \begin{pmatrix}
		K \\ 
		k
		\end{pmatrix}  k^N$$
	
	\subsection{K-means}
	
	Cet algorithme effectue un partitionnement avec un nombre $k$ fixé de clusters. On utilise la distance euclidienne entre deux objets, et on va chercher à minimiser la somme des variances intra-cluster :
	
	$$W(C) = \frac{1}{2} \sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(i') = k} \Vert x_i - x_{i'} \Vert^2$$ 
	
	Le fait d'utiliser une distance euclidienne permet de réécrire la fonction et d'être plus efficace en passant d'un calcul quadratique par rapport au nombre de cluster à un calcul linéaire sur les points :
	
	$$W(C) = \sum_{k = 1}^K N_k \sum_{C(i) = k} \Vert x_i - \overline{x}_k \Vert^2$$
	
	avec $\overline{x}_k = (\overline{x}_{1k}, \dots, \overline{x}_{pk})$ le centre du cluster $k$ et $N_k$ le nombre de points dans le cluster $k$ :
	
	$$N_k = \sum_{i = 1}^N I(C(i) = k)$$
	
	Cela revient donc à un problème d'optimisation :
	
	$$min_{C, \ens{m_k}^K_1} \sum_{k = 1}^K N_k \sum_{C(i) = k} \Vert x_i - m_k \Vert^2$$
	
	On a deux degrés de liberté : les clusters des points et les centres de masse $m_k$.
	
	Si $C$ est fixé (donc si on sait à quel cluster appartient chaque point), les $m_k$ sont triviaux à calculer. Si les centres de masse sont fixés, on va chercher les plus proches voisins :
	
	$$C(i) = arg \max_k \Vert x_i - m_k \Vert^2$$
	
	
		\subsubsection{Algorithme}
		
		\begin{enumerate}
			\item On assigne aléatoirement chaque point à un cluster OU on répartit les centres de masse aléatoirement
			\item Itérativement :
			
			\begin{itemize}
				\item calculer les moyennes des clusters $\ens{m_1 , \dots , m_K}$
				\item pour ces moyennes, assigner chaque observation à la moyenne de cluster la plus proche :
				
				$$C(i) = arg \min_{1 \leq k \leq K} \Vert x_i - m_k \Vert^2$$
			\end{itemize}
		\end{enumerate}
		
		On s'arrête lorsqu'il n'y a plus de changement ; à chaque itération, la somme va diminuer.
		
		\dessin{48}
		
		\dessin{49}
		
		\dessin{50}
		
		Chaque étape réduit l'éparpillement dans les clusters, donc la convergence est assurée, mais uniquement vers un optimum local. De plus, chacune de ces étapes est linéaire par rapport au nombre d'objets, alors qu'avec le clustering hiérarchique il fallait considérer toutes les paires d'objets.
		
		Vu le départ aléatoire de l'algorithme, on pourrait avoir plusieurs solutions différentes. La solution est de redémarrer l'algorithme plusieurs fois.
		
		L'algorithme des k-means peut être utilisé pour de la compression, en découpant une image en blocs et l'appliquant dessus.
		
		\dessin{54}
		
		\begin{center}
		\begin{tabular}{|c|c|c|c||c|c|}
		\hline
		$p_1$ & $p_2$ & $p_3$ & $p_4$ & C & $m_k$ \\ 
		\hline\hline
		0 & 255 & 128 & 255 & 1 & $m_1$ \\ 
		\hline 
		$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & 2 & $m_2$ \\ 
		\hline 
		  &   &   &   & 1 & $\vdots$ \\ 
		\hline 
		 &   &   &   & 3 &  \\ 
		\hline 
		 &  &  &  & 4 &  \\ 
		\hline 
		\end{tabular} 
		\end{center}
		
		\subsubsection{K-medoids}
		
		Il s'agit d'une extension des k-means qui permet d'utiliser n'importe quelle mesure de distance. 
		
		\dessinS{51}{.5}
		
		Les k-medoids sont par contre beaucoup plus lents : dans le calcul de l'expression, pour chaque cluster, on doit vérifier toutes les paires de point, ce qui donne un algorithme quadratique.
		
		\subsubsection{Forces et faiblesses}
		
		\begin{itemize}
			\item[+] simple et facile à comprendre
			\item[+] on peut clusteriser n'importe quel nouveau point (contrairement au clustering hiérarchique)
			\item[+] bonne motivation théorique
			\item[-] il faut fixer le nombre de clusters
			\item[-] sensible au choix initial des centres des clusters
			\item[-] sensible aux données isolées (outliers)
		\end{itemize}
	
	\subsection{Self-Organizing Maps}
	
	C'est une méthode similaire au k-means, mais avec des contraintes supplémentaires : les clusters sont donnés sous forme de matrice (à une ou deux dimensions).
	
	\dessinS{52}{.5}
	
		\subsubsection{Algorithme}
		
		Itérativement :
		\begin{itemize}
			\item prendre $P$ données aléatoirement		
			\item déplacer tous les noeuds dans la direction de $P$ : plus un noeud est dans la topologie mieux c'est (et inversement)
			\item diminuer la quantité de mouvement autorisée
		\end{itemize}
		
		
	\subsection{Nombre de cluster}
	
	La question est de savoir quand s'arrêter dans le clustering hiérarchique et comment choisir $k$ pour les k-means et les SOMs. On a un phénomène d'overfitting, comme en apprentissage supervisé :
	
	\begin{itemize}
		\item on sur-apprend les données s'il y a trop de clusters. On arrive ainsi à trouvers des clusters qui n'existent pas dans les données à cause du bruit
		\item on sous-apprend les données s'il y a trop peu de clusters, on passe ainsi à côté de clusters pertinents
	\end{itemize}
	
	Il n'est cependant pas possible, contrairement à l'apprentissage supervisé, d'effectuer une validation croisée.
	
	Un choix de nombre de cluster consiste à prendre le point d'inflexion de la courbe de variance intra-cluster.
	
	\dessin{53}
	
	
	Autres possibilités :
	
	\begin{itemize}
		\item utiliser des indices internes : sélectionner $k$ qui minimise/maximise les distances intra- et extra-cluster
		\item gap statistic : utiliser une méthode qui compare un indice interne avec ce qu'on aurait obtenu avec des données aléatoire, et choisir le $k$ qui minimise cette différence
		\item stabilité : sélectionner le $k$ qui conduit à des clusters stables (calculés avec une analyse bootstrap)
	\end{itemize}
	
	\subsection{Sélection de features}
	
	La sélection de composantes peut améliorer le clustering en diminuant le bruit (et le temps de calcul).
	
	Par exemple, on va garder les 100 variables avec le plus de variance.
	
	Du clustering après une sélection de features supervisée doit être évitée, car on retrouvera toujours la classification, vu que c'est le critère utilisé pour sélectionner les variables.
	
	\dessinS{115}{.45}
	

\section{Réduction de dimensionnalité}

On va chercher à diminuer la dimensionnalité de l'espace des données. On a plusieurs possibilités :

\begin{itemize}
	\item une sélection de features : on trouve un sous-ensemble des variables originales : $X_i' = X_j$ pour certains $j$
	\item une extraction de features : on transforme l'espace original en un espace de plus petite dimension : $X_i' = f(X_1, \dots , X_p)$
	\item des méthodes linéaires : $f(X_1, \dots , X_p) = w_0 + w_1 X_1 + \dots + w_p X_p$
\end{itemize}

Dans tous les cas, on doit pouvoir reconstruire la base de données d'origine à partir de la version compacte. Les objectifs de la réduction de dimensionnalité sont de

\begin{itemize}
	\item réduire la dimensionnalité avant de faire passer les données dans d'autres méthodes
	\item choisir les variables les plus utiles (qui apportent le plus d'information)
	\item compresser les données
	\item visualiser des données multidimensionnelles, pour identifier des groupes d'objets et identifier des \textit{outliers}.
\end{itemize}

	\subsection{PCA}
	
	L'analyse en composantes principales (PCA) est une méthode linéaire qui transforme un grand nombre de variables en un petit ensemble de variables non corrélées que l'on appelle composantes principales. L'idée est de mapper les points dans de petites dimensions, tout en préservant au maximum la variances des données.
	
	On va chercher une direction (composante principale) afin, après projection des données, de maximiser l'étalement. On peut prendre une deuxième composante, afin de conserver plus d'informations, au cas où on aurait trop d'approximations. Chaque point est ainsi codé par sa distance par rapport à une origine.
	
	\dessinS{78}{.5}
		
	\dessinS{79}{.5}
	
	Cette méthode est très efficace quand il y a beaucoup de corrélation entre les variables (donc de la redondance).
	
	\dessin{83}
	
	\subsection{Approche mathématique}
	
	On a deux formulations du problème possibles :
	
	\begin{itemize}
		\item maximisation de la variance : on trouve les directions qui maximisent la variance des données projetées.
		\item minimisation de l'erreur : on minimise l'erreur de reconstruction des données projetées.
	\end{itemize}
	
	Considérons un ensemble d'observations $\ens{x_n}$, $n = 1, \dots , N$, avec $x_n$ un vecteur de dimension $D$. On veut trouver la direction unitaire $u_1$ qui maximise la variance de la projection :
	
	$$arg \max_{u_1} \frac{1}{N} \sum_{n = 1}^N \Vert u_1^T x_n - u_1^T \xo \Vert^2 = u_1^TCu_1$$
	
	avec
	
	$$\Vert u_1 \Vert = u_1^Tu_1 = 1$$
	$$C = \frac{1}{N} \sum_{i = 1}^N (x_n - \xo)(x_n - \xo)^T$$
	
	$\Vert u_1^T x_n - u_1^T \xo \Vert^2$ est la variance du vecteur considéré, car on a
	
	$$(\sum_i u_1^Tx_i - \underbrace{\frac{1}{N} \sum_i u_1^Tx_i}_{-u_1^T \underbrace{\frac{1}{N} \sum_i x_i}_{\xo}})^2$$
	
	La contrainte de vecteur unitaire ($\Vert u_1 \Vert = 1$) est nécessaire, sinon le vecteur aurait une variance infinie.
	
	Si on introduit le lagrangien :
	
	$$u_1^T Cu_1 + \lambda_1(1 - u_1^Tu_1)$$
	
	$$\frac{\vartheta}{\vartheta u_1} = 0 \Leftrightarrow Cu_1 - \lambda u_1 = 0 \Leftrightarrow Cu_1 = \lambda u_1$$
	
	$u_1$ est un vecteur propre de $C$. Si on multiplie les deux membres à gauche par $u_1^T$, on a
	
	$$u_1^TCu_1 = \lambda u_1^Tu_1 = \lambda \Vert u_1 \Vert = \lambda_1$$
	
	On a donc que $u_1$ est un vecteur propre correspondant à la plus grande valeur propre $\lambda_1$.
	
	
	Rechercher la seconde composante est similaire, mais on ajoute une contrainte d'orthogonalité. Si on généralise à $M$ composantes, on a que la $M + 1$ème composante est obtenue en maximisant 
	
	$$u_{M + 1}^T C u_{M + 1}$$
	
	avec les contraintes
	
	$$u_{M + 1}^T u_{M + 1} = 1$$
	$$u_{M + 1} u_i = 0, \: \forall i = 1, \dots , M + 1$$
	
	Avec le multiplicateur lagrangien :
	
	$$u_{M + 1}^T Cu_{M + 1} + \lambda_{M + 1} (1 - u_{M + 1}^T u_{M + 1}) + \sum_{i = 1}^M \eta_i u_{M + 1}^T u_i$$
	
	On a l'optimum
	
	$$0 = 2 C u_{M + 1} - 2 \lambda_{M + 1} u_{M + 1}  +\sum_{i=1}^M \eta_i u_i$$
	
	Si on multiplie à gauche par $u_i^T$, on a
	
	$$0 = 2u_i^T C u_{M + 1} - 2 \lambda_{M + 1} \underbrace{u_i^T u_{M + 1}}_{0}  +\sum_{i=1}^M \eta_i u_i^T u_i$$
	
	Or, 
	$$u_i^TCu_{M + 1} = (C^Tu_i)^Tu_{M + 1} = (Cu_i)^T u_{M + 1} = (\lambda_i u_i)^T u_{M + 1} = \lambda_i u_i^T u_{M + 1} = 0$$
	
	On a donc que $\eta_i = 0$ et donc
	
	$$C u_{M + 1} = \lambda_{M + 1} u_{M + 1}$$
	
	$u_{M + 1}^T$ est le vecteur propre de la $M + 1$ème plus grande valeur propre.
	
	La $i$ème composante principale pour des objets $x_k$ est donnée par $x_{ji}' = u_i^Tx_j$. La reconstruction de l'entrée se fait par
	
	$$\xu_j = \sum_{i = 1}^M x_{ji}' u_i = \sum_{i = 1}^M(u_i^Tx_j)u_i$$
	
	PCA minimise également l'erreur de reconstruction :
	
	$$arg \max_{u_1, \dots , u_M} \frac{1}{N} \sum_{i = 1}^N \Vert x_j - \xh_j \Vert^2$$
	
	\dessinS{80}{.4}
	
	Au final, on obtient un tableau de la forme
	
	\begin{center}
	\begin{tabular}{c|cccc}
	\hline 
	  & 1 & 2 & \dots & $k$ \\ 
	\hline 
	$\xu_1$ & $\uu_1^T\xu_1$ & $\uu_2^T\xu_1$ & \dots & $\uu_k^T\xu_1$ \\ 
	$\xu_2$ &   &   &   &   \\ 
	\vdots &   &   &   &   \\ 
	\end{tabular} 
	\end{center}
	
	Les $u_k$ sont les vecteurs propres de la matrice de covariance, les valeurs propres sont quant à elles les variances de la projection.
	
	\subsection{Etude des composantes}
	
	\dessinS{81}{.5}
	
	Le poids de chaque variable donne une idée de son importance dans une composante, et peut être utilisé pour de la sélection de features. Pour chaque composante, on a une mesure du pourcentage de la variance des données initiales qu'elles contiennent.
	
	La question est de savoir combien de composantes il faut prendre. Pour cela, on trace le scree plot, les valeurs propres ($\Leftrightarrow$ variances) de chaque composante en ordre décroissant.
	
	\dessin{82}
	
	On supprime les composantes avec des valeurs propres inférieures à 1 et on prend $k$ au point d'inflexion de la courbe, à partir duquel les débris commencent à s'accumuler.
	
	\subsection{Limitations}
	
	PCA peut retrouver visuellement des groupes de données, mais s'il échoue à retrouver ces groupes, on ne peut rien conclure, et c'est indirect par rapport aux méthodes de clustering.
	
	PCA peut être utilisé pour de la sélection de composantes, mais les premières composantes ne sont pas forcément liées à la sortie et les méthodes de sélection de features supervisées fonctionnent mieux. De ce fait, PCA ne doit être considéré que comme un outil d'exploration.
	
	\subsection{Extensions de PCA}
	
		\subsubsection{Kernel PCA}
		
		Il s'agit d'une technique d'extraction de features basée sur le noyautage de PCA.
		
		\dessin{84}
		
		\subsubsection{Sparse PCA}
		
		On cherche les composantes avec des données creuses, avec peu de composantes avec des poids non nuls. On utilise des pénalisations différentes, comme par exemple L1 (LASSO).
		
	\subsection{Autres techniques de réduction de dimension}
	
		\subsubsection{Independent Component Analysis (ICA)}
		
		On ne se force pas à utiliser des composantes orthogonales.
		
		\dessin{85}
		
		\subsubsection{Auto-encodeur avec des réseaux de neurones}
		
		\dessin{86}
		
		\subsubsection{Multi-dimensional scaling (MDS)}
		
		On cherche des nouvelles coordonnées telles que certaines distances sont respectées (au sens des moindres carrés).
		\dessinS{87}{.4}
		
\section{Autres méthodes non-supervisées}

\begin{itemize}
	\item règles associatives
	\item estimation de densitée :
	
	\begin{itemize}
		\item modèles de mixture
		\item réseau bayesiens
	\end{itemize}
\end{itemize}