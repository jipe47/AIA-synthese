\chapter{Évaluation de modèles}

On aimerait estimer les performances d'un modèle ayant appris sur un ensemble de données (de taille $N$). Le but est de

\begin{itemize}
	\item sélectionner un ou plusieurs modèles (ex : déterminer la bonne complexité ou choisir entre différents algorithmes d'apprentissage).
	\item évaluer les modèles, afin d'estimer les performances sur des nouvelles données.
\end{itemize}


\section{Erreurs}
	\subsection{Erreur de resubstitution}
	
	L'erreur de re-substitution ($\LS$ error) est l'erreur obtenue en appliquant le modèle à l'échantillon d'apprentissage. Plus le modèle est complexe et plus cette erreur sera proche de 0.
	
	Cette erreur est un mauvais indicateur des performances d'un modèle car il est beaucoup trop optimiste.
	
	\subsection{Erreur de généralisation}

	L'erreur de généralisation est l'erreur obtenue sur la prédiction de nouvelles données. Si $\fh_{\LS}$ est la fonction apprise sur un échantillon $\LS$, l'erreur de généralisation est décrite par

	$$\exy\ens{L(y, \fh_{\LS}(x))}$$

	avec $L(., .)$ une fonction de perte qui mesure la différence entre les arguments.

	Cette erreur est à différencier de l'erreur de généralisation attendue (expected generalization error) sur un $\LS$ aléatoire de taille $N$, décrite par $E_{\LS} \ens{\text{Err}_{\LS}} = \els{\exy{L(y, \fh_{\LS}(x))}}$.

\section{Méthodes d'évaluation}
	\subsection{Méthode test set}

	On suppose qu'on dispose de beaucoup de données, que $N$ est grand. On divise la base de données en deux parties, une qui servira d'ensemble d'apprentissage et l'autre d'ensemble de test (par ex 70\%, 30\%). La méthode est la suivante :

	\begin{itemize}
		\item on apprend le modèle sur $\LS$
		\item on le test sur $\TS$
		\item l'estimation qui en résulte est une estimation de l'erreur d'un modèle qui aurait appris sur toute la base de données.
	\end{itemize}
	
	Avantages et inconvénients :
	
	\begin{itemize}
		\item[+] très simple à mettre en place ;
		\item[+] efficace ;
		\item[-] les données d'apprentissage sont moindres ;
		\item[-] instable lorsque la base de données est petite.
	\end{itemize}
	
	\dessin{65}
	
	\subsection{K-fold}
	
	La méthode du test-set n'est pas fiable sur une petite base de données car elle est basée sur un petit échantillon d'une base de données déjà réduite. De plus, on utilise l'estimation du modèle construit comme une estimation pour toute la base de données, or lorsque la base de données est très petite, apprendre le modèle sur toute la base ou sur une partie change fortement les résultats.
	
	On peut tracer la courbe d'apprentissage où on confronte les performances à la taille de l'échantillon d'apprentissage. On voit qu'il faut que le $\LS$ soit d'une taille minimale si on veut avoir des résultats pertinents.
	
	\dessin{63}
	
	On utilise pour cela la validation k-fold : on divise aléatoirement la base de données en $k$ sous-ensembles (typiquement $k = 10$).
	
	\dessin{64}
	
	La méthode est la suivante :
	
	\begin{itemize}
		\item pour chaque sous-ensemble
		\begin{itemize}
			\item apprendre le modèle sur les objets qui ne sont pas dans le sous-ensemble
			\item calculer les prédictions du modèle sur les points du sous-ensemble
		\end{itemize}
		\item reporter l'erreur moyenne sur ces prédictions
	\end{itemize}
	
	Lorsque $k = N$, la méthode s'appelle une validation croisée \textit{leave-one-out}.
	
	Le choix de $k$ est très important et conduit à différents avantages et inconvénients :
	
	\begin{itemize}
		\item si $k = N$ (leave-one out) :
		\begin{itemize}
			\item[+] non-biaisé : enlever un objet ne change pas trop la taille de l'échantillon d'apprentissage
			\item[-] grande variance : on dépend énormément de la base de données
			\item[-] lent : il faut entraîner $N$ modèles
		\end{itemize}
		\item si $k = 5, 10$ :
		
		\begin{itemize}
			\item[+] petite variance et rapide : on n'a que $5-10$ modèles sur peu de données
			\item[-] potentiellement biaisé (voir courbe d'apprentissage)
		\end{itemize}
	\end{itemize}
	
	
\subsection{Impact de la complexité d'un modèle avec une validation croisée}

	\dessin{7}

	Il vaut mieux utiliser une petite complexité si on ne dispose pas de beaucoup d'échantillons.

	Avec une complexité fixe, on obtient le comportement suivant.

	\dessin{8}

	Le contrôle de la complexité s'appelle la régulation ou le lissage (smoothing). Il peut être contrôlé de plusieurs façons :

	\begin{itemize}
		\item en variant la taille de l'espace d'hypothèse, autrement dit le nombre de modèles candidats, la valeur des paramètres, etc ;
		\item avec un critère de performance : on oppose les performances de l'ensemble d'apprentissage et la valeur des paramètres, autrement dit on minimise
	
		$$\text{Err}(LS) + \lambda C(\text{model})$$
	
		\item avec des algorithmes d'optimisation : le nombre d'itération, la nature du problème d'optimisation, etc.
	\end{itemize}

	Le choix d'un algorithme se fait en comparant leur taux d'erreur avec une méthode de type cross-validation sur des sous-échantillons. Ensuite, l'algorithme avec le plus petit taux est utilisé comme modèle prédicatif sur toutes les données.

	L'utilisation intensive de la méthode CV peut entraîner du sur-apprentissage. En effet, plus on compare de modèles complexes, plus on a une chance d'en trouver un qui convient pour les données. La solution pour éviter cela est de réserver un ensemble de test additionnel (ou de les générer), et de l'utiliser pour tester les performances du modèle final.
	
	\subsection{Bootstrap}
	
	Un échantillon bootstrap est un échantillon avec un remplacement : des objets n'apparaissent pas et d'autres apparaissent plusieurs fois.
	
	\dessin{66}
	
	On a alors que
	
	$$P(o_i \in \text{ bootstrap}) = 1 - (1 - \frac{1}{N})^N \approx 1 - \frac{1}{e} = 0.632$$
	
	L'idée est d'utiliser les 30\% de données qu'il reste comme $\TS$. On peut alors estimer l'erreur de bootstrap :
	
	\begin{itemize}
		\item pour $i = 1$ jusqu'à $B$ :
		
		\begin{itemize}
			\item prendre un échantillon de boostrap $B_i$ de la base de données
			\item apprendre un modèle $f_i$ sur cet échantillon
		\end{itemize}
		
		\item pour chaque objet, calculer l'erreur de tous les modèles qui ont été construis sans lui (environ 30\%)
		\item moyenner sur tous les objets
	\end{itemize}
	
	Des améliorations existent :
	
	\begin{itemize}
		\item $.632$ bootstrap : correction pour la courbe d'apprentissage
		\item $.632+$ bootstrap : correction pour le sur-apprentissage
	\end{itemize}
	
	\subsection{Erreurs de test conditionnelles et erreurs de test attendues}
	
	Pour un modèle $\hat{f}_\LS$ donné, on a l'erreur de test conditionnelle :
	
	$$\text{Err}_\LS = \exy{L(y, \hat{f}_\LS(x))}$$
	
	On a l'erreur attendue :
	
	$$\els{\text{Err}_\LS} = \els{\exy{L(y, \hat{f}_\LS(x))}}$$
	
	Seule la méthode test set estime la première erreur, la validation croisée permet quant à elle d'estimer la seconde car on fait de l'apprentissage sur des $\LS$ différents.
	
\section{Méthodes de sélection}

Le but, pour une base de données de $N$ objets, est de déterminer le meilleur modèle possible et d'estimer l'erreur des prédictions. La méthodologie dépend encore une fois de la taille de la base.

Le choix d'une mesure de l'erreur ou de la qualité dépend fortement de l'application. On peut également définir des autres critères pour évaluer un modèle.

	\subsection{Méthode test set}
	
	Si la base de données est grande, on divise aléatoirement l'ensemble d'apprentissage en trois parties : un ensemble d'apprentissage $\LS$, un ensemble de validation $\VS$ et un ensemble de test $\TS$ (par exemple 50\%, 25\% et 25\%).
	
	\dessin{67}
	
	La méthode est la suivante :
	
	\begin{itemize}
		\item apprendre les modèles à comparer sur $\LS$
		\item sélectionner le meilleur en basant les performances sur $\VS$
		\item le ré-entraîner sur $\LS + \VS$
		\item le tester sur $\TS$, afin d'avoir une estimation des performances
		\item le ré-entraîner sur $\LS + \VS + \TS$, afin d'avoir le modèle final
	\end{itemize}
	
	$\VS$ permet de sélectionner le modèle. Une fois que c'est fait, on réapprend sur $\LS$ et $\VS$ et on teste sur $\TS$. Le choix de la meilleure méthode (apprise sur $\LS$ et testée sur $\VS$) n'entraîne pas de sur-apprentissage, mais il y a tout de même un risque s'il y a trop de méthodes à tester (on en aurait une qui donne des bons résultats par coup de chance), ce qui pourrait donner une grosse erreur sur $\TS$. La solution est d'ajouter une nouvelle boucle de validation croisée ; tout choix d'échantillon peut créer un biais.
	
	\subsection{Validation croisée}
	
	On utilise deux étapes de validation croisée en k-fold.
	
	\dessin{68}
	
	CV1 est utilisé pour évaluer le modèle final, tandis que CV2 est utilisé pour la sélection du modèle.
	
	On peut également combiner la méthode test set et la validation croisée.
	
	\dessin{69}
	
	Les deux étapes sont nécessaires, car plus on compare des modèles et plus la probabilité de tomber par chance sur celui qui donne des bons résultats augmente. Les erreurs sur $\VS$ ou sur CV2 sont alors en général trop optimistes.
	
	Illustration :
	
	\dessinS{70}{.4}
	
	\subsection{Méthodes analytiques}
	
	On cherche le modèle qui minimise un critère de la forme
	
	$$\text{Err}(\LS) + G(\text{complexité})$$
	
	où $G$ est une fonction monotone croissante. Le critère est dérivé de preuves théoriques.
	
	L'avantage est qu'il n'y a pas de re-entraînement, par contre on ne peut l'utiliser que pour la sélection de modèle, et on pourrait manquer le vrai optimum dans le cas d'échantillons finis.
	
\section{Biais de sélection}
	
	En général, n'importe quel choix fait en utilisant la sortie doit être dans une boucle de validation croisée, car il peut potentiellement amener à du sur-apprentissage. En effet, un gain d'apprentissage sur un $\LS$ peut ne pas se répercuter sur des nouvelles données à prédire.
	
	\dessinS{71}{.4}
		
\section{Mesure de performance}

	\subsection{Classification binaire}
	
	Les résultats peuvent être résumés dans un tableau de contingence/matrice de confusion.
	
	\dessinS{72}{.5}
	
	On définit alors
	
	\begin{eqnarray*}
	\text{Taux d'erreur (error rate)} & = & \frac{FP + FN}{N + P} \\
	\text{Précision (accuracy)} & = & \frac{TP + TN}{N + P} = 1 - \text{ error rate}
	\end{eqnarray*}
	
	Le taux d'erreur est cependant limité : on n'a pas d'informations sur la distribution des erreurs sur les classes, et il est sensible aux changement dans la distribution des classes dans l'échantillon de test.
	
	\dessinS{73}{.45}
	
	Dans le cadre de diagnostiques médicaux, on utilise des mesures plus appropriées :
	
	\begin{eqnarray*}
	\text{Sensibilité (sensitivity/recall)} & = & \frac{TP}{P} \\
	\text{Spécificité (specificity)} & = & \frac{TN}{TN + FP} = 1 - \frac{FP}{N}
	\end{eqnarray*}
		
	La sensibilité permet de détecter le maximum de positif (plus elle est grande et plus on est sûr de détecter les cas positifs), tandis que la spécificité détecte le maximum de négatif. L'avantage de ces mesures est qu'elles ne dépendent pas de la proportion d'objets positifs ou négatifs.
	
	Ces mesures peuvent être portées sur une courbe ROC (Receiver operating characteristic). Si la sortie de l'algorithme d'apprentissage est un nombre (par exemple, la probabilité d'appartenir à une classe), on peut utiliser un seuil afin de régler la sensibilité et la spécificité.
	
	\dessinS{74}{.5}
	
	Le meilleur algorithme est le D. Si un modèle retourne toujours la classe positive, il se trouvera dans le coin supérieur droit. S'il retourne toujours la classe négative, il sera dans le coin inférieur gauche. S'il renvoie une valeur au hasard, il sera situé au centre ; la diagonale représente les choix aléatoires biaisés.
	
	Si on se trouve dans la diagonale inférieure, on peut inverser les réponses ($+ \Rightarrow -$, $- \Rightarrow +$) pour revenir dans la diagonale supérieure.
	
	\dessinS{75}{.5}
	
	On peut résumer une courbe ROC avec un nombre : l'AUC, la surface en dessous de la courbe. On peut l'interpréter comme la probabilité que deux objets choisis aléatoirement dans l'échantillon sont correctement ordonnés par le modèle, c'est-à-dire que le positif a un plus haut score que le négatif.
	
	On utilise d'autre mesures :
	
	\begin{itemize}
		\item la précision, la proportion de bonnes prédictions parmi les prédictions positives
		\item le \textit{recall}, la proportion de positifs qui sont détectés
		\item la \textit{f-mesure}
	\end{itemize}
	
	\begin{eqnarray*}
	\text{Précision} & = & \frac{TP}{TP + FP} \\
	\text{Recall} & = & \frac{TP}{TP + FN} \\
	\text{F-mesure} & = & \frac{2 * \text{précision} * \text{recall}}{\text{précision} + \text{recall}}
	\end{eqnarray*}
	
	\dessinS{76}{.5}
	
	Le meilleur algorithme aura une courbe ROC de la forme {\pigpenfont I}, tandis que la courbe de recall aura la forme {\pigpenfont C}.
	
	\subsection{Régression}
	
		A partir du moment où on a plus d'une classe, on utilise un taux d'erreur plutôt qu'une courbe ROC.
	
		\subsubsection{Erreur quadratique}
		
		$$\frac{1}{N} \sum_{i = 1}^N (y_i - \yh_i)^2$$
		
		Le carré permet de pénaliser les très mauvaises prédictions.
		
		\subsubsection{Erreur absolue moyenne}
		
		$$\frac{1}{N} \sum_{i = 1}^N \vert y_i - \yh_i \vert$$
		
		\subsubsection{Corrélation de Pearson}
		
		$$\frac{\sum_i (y_i - \frac{1}{N} \sum_j y_j)(\yh_i - \frac{1}{N} \sum_j \yh_j)}{(N - 1)s_y s_{\yh} }$$
		 		
		\subsubsection{Corrélation de rang de Spearman}
		
		$$1 - \frac{6 \sum_i d_i^2}{N(N^2 - 1)}$$
		
		avec $d_i$ la différence de rang de $y_i$ et $\yh_i$, le rang étant l'ordre obtenu après un tri des valeurs.
		
	\subsection{Mesures de performances pour l'entraînement}
	
	Les mesures de performances pour l'entraînement peuvent être différentes des mesures de performances de test. Il y a plusieurs raisons à cela :
	
	\begin{itemize}
		\item algorithmiquement, une mesure dérivable est soumise à une optimisation de gradient (par exemple le taux d'erreur et l'erreur absolue moyenne ne sont pas dérivables, l'AUC n'est pas décomposable)
		\item le sur-apprentissage : pour l'entraînement, la perte incorpore souvent un terme de pénalité pour la complexité du modèle (ce qui est inutile au moment du test). De plus, certaines mesures sont moins promptes au sur-apprentissage (par exemple la marge).
	\end{itemize}